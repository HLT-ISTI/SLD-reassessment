{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, fixed, IntProgress\n",
    "from plotly_colors import colors\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from load_data import load_em_history, LoadedData, get_measures_from_history, labels_dict, \\\n",
    "    get_measures_mean_across_iterations, Measures\n",
    "import itertools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_em_results(arg: LoadedData, fig, row, col):\n",
    "    if arg is None:\n",
    "        return \n",
    "    with open(arg.file_name, 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    measures = get_measures_from_history(history)\n",
    "    \n",
    "    for name, measure in measures:\n",
    "        random.seed(a=name)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=measure,\n",
    "            x=np.array(range(len(measure))),\n",
    "            mode='lines',\n",
    "            name=name,\n",
    "            visible=\"legendonly\",\n",
    "            legendgroup=name,\n",
    "            showlegend=row == 1 and col ==1,\n",
    "            line={'color': random.choice(colors)},\n",
    "        ), row=row, col=col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='plot_one'></a>\n",
    "## Compare classifiers on one iteration\n",
    "Plot one or more classifier results. Left click to select one, hold `ctrl` key + left click to select more than one.\n",
    "You can select only one iteration at a time. If instead you wish to see the results mean across all iterations, please\n",
    "go to [Compare classifiers for all iterations (one or more measures)](#plot_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_multiple_em_results(args):\n",
    "    if args is None:\n",
    "        return \n",
    "    if type(args) != list:\n",
    "        args = [args]\n",
    "    global fig\n",
    "    if fig is not None and type(fig) is go.FigureWidget:\n",
    "        fig.close()\n",
    "        \n",
    "    progress = IntProgress(min=0, max=100, description=\"Loading: 0%\")\n",
    "    display(progress)\n",
    "    fig = make_subplots(rows=math.ceil(len(args) / 2) if len(args) > 1 else 1, cols=2, \n",
    "                        subplot_titles=[arg.clf_name for arg in args])\n",
    "    \n",
    "    for i, data in enumerate(args):\n",
    "        plot_em_results(data, fig, row=math.ceil((i+1) / 2), col=1 if (i+1) % 2 != 0 else 2)\n",
    "        progress.value += 100 / len(args)\n",
    "        progress.description = f\"Loading: {progress.value}%\"\n",
    "        \n",
    "    fig.update_layout(autosize=False, width=900, \n",
    "                      height=400*(math.log2(len(args)) if len(args) > 1 else 1))\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    progress.close()\n",
    "    fig = go.FigureWidget(fig)\n",
    "    display(fig)\n",
    "    \n",
    "    \n",
    "def select_random_execution(clf_names, data):\n",
    "    if not clf_names:\n",
    "        return \n",
    "    filtered_list = sorted(filter(lambda el: el.clf_name in clf_names, data), key=lambda k: k.it)\n",
    "    it_list = list()\n",
    "    for it, group in itertools.groupby(filtered_list, lambda el: el.it):\n",
    "        it_list.append((f\"Iteration: {it}\", list(group)))\n",
    "        \n",
    "    wid = widgets.Select(options=it_list, description=\"Random run\")\n",
    "    w_disp = interactive(plot_multiple_em_results, args=wid)\n",
    "    display(w_disp)\n",
    "        \n",
    "    \n",
    "fig = None  # storing last figure so that we can later close it to free resources\n",
    "em_data = list(load_em_history())\n",
    "classifiers = {f\"{el.clf_name}{'_' + el.dataset if el.dataset else ''}\" for el in em_data}\n",
    "w = widgets.SelectMultiple(options=sorted(list(classifiers)), description=\"Classifiers\")\n",
    "\n",
    "w_disp = interactive(select_random_execution, clf_names=w, data=fixed(em_data))\n",
    "display(w_disp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='plot_mean'></a>\n",
    "## Compare classifiers for all iterations (one or more metrics)\n",
    "In this section you can select one or more metrics and visualize its/their means across all the experimental iterations,\n",
    "for all classifiers. If instead you wish to compare classifiers on one iteration, go to [Compare classifiers on one iteration](#plot_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34e40cf20f94924852ef569a034c9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Metrics', options=(('Abs. err.', 'abs_errors'), ('Brier', 'bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_metrics(metrics, data):\n",
    "    if not metrics:\n",
    "        return \n",
    "    global fig\n",
    "    if fig is not None and type(fig) is go.FigureWidget:\n",
    "        fig.close()\n",
    "        \n",
    "    fig = make_subplots(rows=math.ceil(len(metrics) / 2), cols=2, subplot_titles=[labels_dict[m] for m in metrics], shared_yaxes=True)\n",
    "    progress = IntProgress(min=0, max=100, description=\"Loading: 0%\")\n",
    "    display(progress)\n",
    "    metrics_clf = [(clf_name, get_measures_mean_across_iterations(measures, metrics)) for clf_name, measures in data]\n",
    "        \n",
    "    i = 0\n",
    "    for metric in metrics:\n",
    "        measures = list(map(lambda el: (el[0], el[1].__getattribute__(metric)), metrics_clf))\n",
    "        row = math.ceil((i+1) / 2)\n",
    "        col = 1 if (i+1) % 2 != 0 else 2\n",
    "        picked_colors = set()\n",
    "        max_len = max(len(m[1]) for _, m in measures)\n",
    "        for clf_name, measure in measures:\n",
    "            measure = measure[1]\n",
    "            random.seed(a=hash(clf_name))\n",
    "            color = random.choice(colors)\n",
    "            \n",
    "            # Keep selecting a color until we find one we haven't already used\n",
    "            while color in picked_colors:\n",
    "                color = random.choice(colors)\n",
    "            picked_colors.add(color)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                y=np.pad(measure, (0, max_len - len(measures)), mode='edge'),\n",
    "                x=np.array(range(max_len)),\n",
    "                mode='lines',\n",
    "                name=clf_name,\n",
    "                visible=\"legendonly\",\n",
    "                legendgroup=clf_name,\n",
    "                showlegend=row == 1 and col ==1,\n",
    "                line={'color': color},\n",
    "            ), row=row, col=col)\n",
    "            progress.value += 100 / len(metrics)\n",
    "            progress.description = f\"Loading: {progress.value}%\"\n",
    "        i += 1\n",
    "    \n",
    "    fig.update_layout(autosize=False, width=900, \n",
    "                      height=400*(math.log2(len(metrics)) if len(metrics) > 1 else 1))\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    progress.close()\n",
    "    fig = go.FigureWidget(fig)\n",
    "    display(fig)\n",
    "                    \n",
    "                \n",
    "fig = None\n",
    "with open('./pickles/measures_all_em_rcv1_30-01-20.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "w = widgets.SelectMultiple(options=sorted(map(lambda el: (el[1], el[0]), labels_dict.items())), description=\"Metrics\")\n",
    "w_disp = interactive(plot_metrics, metrics=w, data=fixed(data))\n",
    "display(w_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}